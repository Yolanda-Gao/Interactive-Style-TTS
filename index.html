<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
   <head>
      <meta charset="UTF-8">
      <title align="center">Audio samples for  "INTERACTIVE TEXT-TO-SPEECH VIA SEMI-SUPERVISED STYLE TRANSFER LEARNING"</title>
      <style type="text/css">
        body, input, select, td, li, div, textarea, p {
        	font-size: 11px;
        	line-height: 16px;
        	font-family: verdana, arial, sans-serif;
        }

        body {
        	margin:5px;
        	background-color:white;
        }

        h1 {
        	font-size:16px;
        	font-weight:bold;
        }

        h2 {
        	font-size:14px;
        	font-weight:bold;
        }
      </style>
   </head>
   <body>
      <article>
         <header>
            <h1>Audio samples for "INTERACTIVE TEXT-TO-SPEECH VIA SEMI-SUPERVISED STYLE TRANSFER LEARNING"</h1>
         </header>
      </article>

      <div>
        <h2>Abstract</h2>
        <p> With increasing interests in interactive speech systems, speech emotion recognition and multi-style text-to-speech (TTS) synthesis are becoming increasingly important research areas. In this paper, we combine both. We present a method to extract speech style embeddings from input speech queries and apply this embedding to a customized TTS voice so that the TTS response matches the speaking style of the input query. Specifically, we first train a multi-modal style classification model using acoustic and textual features of speech utterances. Due to a limited amount of labeled data, we combined the emotional recognition dataset: the interactive emotional dyadic motion capture database (IEMOCAP) with a small labeled subset of our internal TTS dataset for style model training. We take the softmax layer from the style classifier as style prediction and then apply this style embedding extraction model to generate soft style labels for our unlabelled internal TTS dataset. With this semi-supervised approach, reliable style embeddings are extracted to train a multi-style TTS system. As a result, we developed a controllable multi-style TTS system whose response matches the given target styles embedding, which could be extracted from the input query or manually assigned.</p>
      </div>

      <h2> Contents </h2>
      <div id="toc_container">
         <ul>
            <b> <a href="Style/index.html">A: Style TTS samples</a> </b><br/>
            <b> <a href="RenderingLevel/index.html">B: Controllable styles </a> </b><br/>
            <b> <a href="Matching/index.html">C: Real-life input queries and responses</a> </b><br/>
         </ul>
      </div>
   </body>
</html>
